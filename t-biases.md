![Biases](https://img.shields.io/badge/t-biases-pink)
* Aka, O., Burke, K., BÃ¤uerle, A., Greer, C., & Mitchell, M. (2021). Measuring Model Biases in the Absence of Ground Truth. DOI:10.1145/3461702.3462557. AIES '21: AAAI/ACM Conference on AI, Ethics, and Society. [[paper](https://arxiv.org/abs/2103.03417)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ðŸ¦œ. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 610-623). doi:10.1145/3442188.3445922 [[paper](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)] ![Model Issues](https://img.shields.io/badge/t-model%20issues-yellow) ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Blodgett, S. L., Barocas, S., DaumÃ© III, H., & Wallach, H. (2020). Language (technology) is power: A critical survey of "bias" in NLP.  In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454â€“5476, Online. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.485. [[paper](https://www.aclweb.org/anthology/2020.acl-main.485)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Mohammad, S. M. (2020, July). Gender gap in natural language processing research: Disparities in authorship and citations. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.702 [[paper](https://www.aclweb.org/anthology/2020.acl-main.702)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Nissim, M., van Noord, R., & van der Goot, R. (2020). Fair is better than sensational: Man is to doctor as woman is to doctor. Computational Linguistics, 46(2), 487-497. doi:10.1162/coli\_a\_00379 [[paper](https://www.aclweb.org/anthology/2020.cl-2.7)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Garimella, A., Banea, C., Hovy, D., & Mihalcea, R. (2019, July). Womenâ€™s syntactic resilience and menâ€™s grammatical luck: Gender-Bias in Part-of-Speech Tagging and Dependency Parsing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 3493-3498). [[paper](https://aclanthology.org/P19-1339.pdf)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Sap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A., & Choi, Y. (2019). Social bias frames: Reasoning about social and power implications of language.  In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5477â€“5490, Online. Association for Computational Linguistics. [[paper](https://aclanthology.org/2020.acl-main.486.pdf)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Curry, A. C., & Rieser, V. (2018, June). # MeToo Alexa: How conversational systems respond to sexual harassment. In Proceedings of the second ACL workshop on ethics in natural language processing (pp. 7-14). [[paper](https://www.aclweb.org/anthology/W18-0802.pdf)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Fort, K., & NÃ©vÃ©ol, A. (2018, January). PrÃ©sence et reprÃ©sentation des femmes dans le traitement automatique des langues en France. In Penser la Recherche en Informatique comme pouvant Ãªtre SituÃ©e, Multidisciplinaire Et GenrÃ©e (PRISME-G). [[paper](https://hal.archives-ouvertes.fr/hal-01683774)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Schluter, N. (2018). The glass ceiling in NLP. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 2793-2798). doi:10.18653/v1/D18-1301 [[paper](https://www.aclweb.org/anthology/D18-1301)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
* Koolen, C. & van Cranenburgh, A. These are not the Stereotypes You are Looking For: Bias and Fairness in Authorial Gender Attribution. In Proceedings of the first ACL workshop on ethics in natural language processing (pp. 12-22). [[paper](https://aclanthology.org/W17-1602.pdf)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![published](https://img.shields.io/badge/type-published-lightgrey)
<!--* Clark, J. (2016). Artificial intelligence has a â€˜sea of dudesâ€™ problem. Bloomberg Technology, 23. [[paper](https://www.bloomberg.com/news/articles/2016-06-23/artificial-intelligence-has-a-sea-of-dudes-problem)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![post](https://img.shields.io/badge/type-post-lightgrey)-->
* Larson, J., Angwin, J., & Parris, T. (2016). Breaking the black box: How machines learn to be racist. _ProPublica_. [[paper](https://www.propublica.org/article/breaking-the-black-box-how-machines-learn-to-be-racist)] ![Biases](https://img.shields.io/badge/t-biases-pink) ![post](https://img.shields.io/badge/type-post-lightgrey)
